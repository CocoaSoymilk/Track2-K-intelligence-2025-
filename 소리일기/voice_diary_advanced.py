import streamlit as st
import pandas as pd
from datetime import datetime
import json
import os
import base64
import tempfile
import warnings
warnings.filterwarnings("ignore")

# --- Lightweight imports first (heavy libs are lazy-imported) ---
import numpy as np
from typing import Dict, List, Optional

# =============================
# Page / App Config
# =============================
st.set_page_config(
    page_title="ì†Œë¦¬ë¡œ ì“°ëŠ” í•˜ë£¨ - ì°¨ì›í˜• ë³´ì¡°ì§€í‘œ ë²„ì „",
    page_icon="ğŸ™ï¸",
    layout="wide",
    initial_sidebar_state="expanded",
)

# =============================
# Utility: Lazy Importers (avoid heavy import unless needed)
# =============================
@st.cache_resource(show_spinner=False)
def get_librosa():
    try:
        import librosa  # type: ignore
        return librosa
    except Exception:
        return None

@st.cache_resource(show_spinner=False)
def get_scipy():
    try:
        import scipy  # type: ignore
        from scipy import signal, stats  # noqa: F401
        return scipy
    except Exception:
        return None

@st.cache_resource(show_spinner=False)
def get_parselmouth():
    try:
        import parselmouth  # type: ignore
        from parselmouth.praat import call  # noqa: F401
        return parselmouth
    except Exception:
        return None

@st.cache_resource(show_spinner=False)
def get_openai_client():
    try:
        import openai  # type: ignore
        # OpenAI Python SDK v1 style client
        if "OPENAI_API_KEY" in st.secrets:
            return openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])  # type: ignore
        elif "openai_api_key" in st.session_state and st.session_state.openai_api_key:
            return openai.OpenAI(api_key=st.session_state.openai_api_key)  # type: ignore
        else:
            return None
    except Exception:
        return None

openai_client = get_openai_client()

# =============================
# Global Session State
# =============================
if "diary_entries" not in st.session_state:
    st.session_state.diary_entries: List[Dict] = []

# ê°œì¸ ê¸°ì¤€ì„ (ì´ˆê¸° 3~5íšŒ í‰ê· ) ì €ì¥ ê³µê°„
if "prosody_baseline" not in st.session_state:
    st.session_state.prosody_baseline: Dict[str, float] = {}

# =============================
# Styles (minimal; keep light for perf)
# =============================
st.markdown(
    """
    <style>
      .main-header{ text-align:center; padding:1.2rem; background:linear-gradient(90deg,#667eea 0%,#764ba2 100%); color:#fff; border-radius:10px; margin-bottom:1rem; }
      .metric-card{ background:#fff; border:1px solid #eee; border-left:4px solid #667eea; border-radius:12px; padding:1rem; box-shadow:0 2px 8px rgba(0,0,0,0.04); }
      .note{ color:#666; font-size:0.85rem; }
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown(
    """
    <div class="main-header">
      <h1>ğŸ™ï¸ ì†Œë¦¬ë¡œ ì“°ëŠ” í•˜ë£¨ â€“ ë³´ì¡°ì§€í‘œ(ì°¨ì›í˜•) ê¸°ë°˜</h1>
      <p>ëª©ì†Œë¦¬ëŠ” ê°ì • ë¼ë²¨ì„ â€˜ê²°ì •â€™í•˜ì§€ ì•Šê³ , ê°ì„±/ê¸´ì¥/ì•ˆì •ì˜ <b>ë³´ì¡° ë‹¨ì„œ</b>ë¡œë§Œ ë°˜ì˜í•©ë‹ˆë‹¤.</p>
    </div>
    """,
    unsafe_allow_html=True,
)

# =============================
# Feature Extraction (Optimized)
# =============================
class VoiceFeatureExtractor:
    """ê²½ëŸ‰/ì•ˆì • ìœ„ì£¼ Prosody Feature Extractor (librosa ìš°ì„ , Praat ì„ íƒ)
    - Lazy import ì‚¬ìš©
    - í•„ìˆ˜ ìµœì†Œ featureë§Œ ê³„ì‚°
    - ì§§ì€ ì˜¤ë””ì˜¤/ì €í’ˆì§ˆ ì˜¤ë””ì˜¤ëŠ” í’ˆì§ˆ ë‚®ê²Œ ì²˜ë¦¬
    """

    def __init__(self, target_sr: int = 22050):
        self.sample_rate = target_sr

    def _load_audio(self, audio_bytes: bytes):
        librosa = get_librosa()
        if not librosa:
            return None, None
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(audio_bytes)
            path = tmp.name
        try:
            y, sr = librosa.load(path, sr=self.sample_rate, mono=True, res_type="kaiser_fast")
            return y, sr
        finally:
            try:
                os.unlink(path)
            except Exception:
                pass

    @st.cache_data(show_spinner=False)
    def _hann(self):
        # small cached window if ever needed later
        return np.hanning(2048)

    def extract(self, audio_bytes: bytes) -> Dict:
        librosa = get_librosa()
        if not librosa:
            return self._default_features()

        try:
            y, sr = self._load_audio(audio_bytes)
            if y is None or y.size == 0:
                return self._default_features()

            duration_sec = max(0.001, float(len(y) / sr))

            # --- Cheap/robust features ---
            # RMS energy (frame-wise -> mean/max)
            rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]
            energy_mean = float(np.mean(rms))
            energy_max = float(np.max(rms))

            # Tempo (coarse) â€“ librosa.beat is relatively heavy; guard by duration
            if duration_sec >= 2.5:
                tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
            else:
                tempo = 0.0

            # Zero Crossing Rate
            zcr = librosa.feature.zero_crossing_rate(y, frame_length=1024, hop_length=256)[0]
            zcr_mean = float(np.mean(zcr))

            # Spectral centroid (coarse brightness)
            sc = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            spectral_centroid_mean = float(np.mean(sc))

            # Pitch proxy (cheap): use piptrack sparsely and choose max per frame
            pitch_mean = 0.0
            pitch_var = 0.0
            try:
                if duration_sec >= 1.0:
                    pitches, mags = librosa.piptrack(y=y, sr=sr, fmin=50, fmax=400)
                    valid = []
                    for t in range(pitches.shape[1]):
                        idx = np.argmax(mags[:, t])
                        p = pitches[idx, t]
                        if p > 0:
                            valid.append(p)
                    if valid:
                        va = np.array(valid, dtype=float)
                        pitch_mean = float(np.mean(va))
                        pitch_var = float(np.std(va) / (np.mean(va) + 1e-6))
            except Exception:
                pass

            # Simple phonation quality proxies (Praat optional)
            parselmouth = get_parselmouth()
            hnr = 15.0
            jitter = 0.012
            if parselmouth:
                try:
                    # write temp wav once more (avoid reusing deleted)
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp2:
                        tmp2.write(audio_bytes)
                        p2 = tmp2.name
                    snd = parselmouth.Sound(p2)
                    # harmonicity (HNR)
                    harm = snd.to_harmonicity_cc()
                    hnr = float(np.nan_to_num(harm.values.mean(), nan=15.0))
                    # jitter local (approx) via PointProcess
                    point_proc = parselmouth.praat.call(snd, "To PointProcess (periodic, cc)", 75, 500)
                    jitter = float(parselmouth.praat.call(point_proc, "Get jitter (local)", 0, 0, 0.0001, 0.02, 1.3))
                except Exception:
                    pass
                finally:
                    try:
                        os.unlink(p2)
                    except Exception:
                        pass

            return {
                "duration_sec": duration_sec,
                "pitch_mean": float(pitch_mean if pitch_mean > 0 else 150.0),
                "pitch_variation": float(pitch_var if pitch_var > 0 else 0.13),
                "energy_mean": float(energy_mean),
                "energy_max": float(energy_max),
                "tempo": float(tempo if tempo > 0 else 110.0),
                "zcr_mean": float(zcr_mean),
                "spectral_centroid_mean": float(spectral_centroid_mean),
                "hnr": float(hnr),
                "jitter": float(jitter),
            }
        except Exception:
            return self._default_features()

    def _default_features(self) -> Dict:
        return {
            "duration_sec": 0.0,
            "pitch_mean": 150.0,
            "pitch_variation": 0.13,
            "energy_mean": 0.08,
            "energy_max": 0.12,
            "tempo": 110.0,
            "zcr_mean": 0.10,
            "spectral_centroid_mean": 2000.0,
            "hnr": 15.0,
            "jitter": 0.012,
        }

# =============================
# Dimensional Voice Cues (Arousal/Tension/Stability + Quality)
# =============================

def prosody_to_dimensions(f: Dict, baseline: Optional[Dict] = None) -> Dict:
    def norm(key: str, val: float) -> float:
        if not baseline or key not in baseline:
            return val
        b = baseline[key]
        return (val / b) if b else val

    pitch = norm("pitch_mean", f.get("pitch_mean", 150.0))  # noqa: F841 (reserved if needed later)
    tempo = norm("tempo", f.get("tempo", 110.0))
    energy = norm("energy_mean", f.get("energy_mean", 0.08))
    hnr = norm("hnr", f.get("hnr", 15.0))
    jitter = f.get("jitter", 0.012)
    zcr = f.get("zcr_mean", 0.10)
    sc = norm("spectral_centroid_mean", f.get("spectral_centroid_mean", 2000.0))

    # Scaled 0~100 dimensional cues (empirical scaling)
    arousal = np.clip(35 + 120 * energy + 0.06 * (tempo - 110) + 0.004 * (sc - 2000), 0, 100)
    tension = np.clip(28 + 120 * jitter + 0.55 * (zcr - 0.10) * 100, 0, 100)
    stability = np.clip(60 + 1.3 * (hnr - 15) - 85 * jitter, 0, 100)

    duration = f.get("duration_sec", 4.0)
    quality = np.clip(
        0.28 * (duration / 8.0) + 0.42 * np.clip((hnr - 10) / 15, 0, 1) + 0.30 * np.clip((energy - 0.06) / 0.20, 0, 1),
        0,
        1,
    )

    return {
        "arousal": float(arousal),
        "tension": float(tension),
        "stability": float(stability),
        "quality": float(quality),
    }


def analyze_voice_as_cues(voice_features: Dict, baseline: Optional[Dict] = None) -> Dict:
    dims = prosody_to_dimensions(voice_features, baseline)
    return {
        "voice_cues": dims,  # arousal/tension/stability/quality
        "voice_features": voice_features,
    }

# =============================
# Text Analysis (LLM or fallback)
# =============================

def analyze_text_with_llm(text: str, voice_cues_for_prompt: Optional[Dict] = None) -> Dict:
    """LLM ì‚¬ìš© ì‹œ: ê°ì • ë¼ë²¨ì€ í…ìŠ¤íŠ¸ë¡œë§Œ íŒë‹¨. ìŒì„±ì€ ë³´ì¡°ì§€í‘œë¡œë§Œ í™œìš©í•˜ë„ë¡ ì§€ì‹œ."""
    if not openai_client:
        return analyze_text_simulation(text)

    cues_text = ""
    if voice_cues_for_prompt:
        cues = voice_cues_for_prompt
        cues_text = (
            f"\n(ì°¸ê³ ìš© ë³´ì¡°ì§€í‘œ) ê°ì„±:{int(cues.get('arousal',0))}, "
            f"ê¸´ì¥:{int(cues.get('tension',0))}, ì•ˆì •:{int(cues.get('stability',0))}, í’ˆì§ˆ:{cues.get('quality',0):.2f}"
        )

    try:
        resp = openai_client.chat.completions.create(
            model="gpt-4o",
            temperature=0.3,
            max_tokens=800,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "ë‹¹ì‹ ì€ ì°¨ë¶„í•œ ë§ˆìŒ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ê°ì • ë¼ë²¨(ê¸°ì¨/ìŠ¬í””/ë¶„ë…¸/ë¶ˆì•ˆ/í‰ì˜¨/ì¤‘ë¦½)ì€ ì˜¤ì§ í…ìŠ¤íŠ¸ë¡œë§Œ íŒë‹¨í•˜ì„¸ìš”. "
                        "ìŒì„± ê´€ë ¨ ì •ë³´ëŠ” ê°ì„±/ê¸´ì¥/ì•ˆì •ì˜ ë³´ì¡°ì§€í‘œë¡œë§Œ ì°¸ê³ í•˜ë©° ë¼ë²¨ì„ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.\n"
                        "JSONìœ¼ë¡œë§Œ ì‘ë‹µ: {\n"
                        "  \"emotions\": [..ìµœëŒ€3ê°œ..],\n"
                        "  \"stress_level\": 0-100,\n"
                        "  \"energy_level\": 0-100,\n"
                        "  \"mood_score\": -70~70,\n"
                        "  \"summary\": \"í•œë‘ ë¬¸ì¥\",\n"
                        "  \"keywords\": [..],\n"
                        "  \"tone\": \"ê¸ì •ì /ì¤‘ë¦½ì /ë¶€ì •ì \",\n"
                        "  \"confidence\": 0.0-1.0\n"
                        "}"
                    ),
                },
                {"role": "user", "content": f"ì˜¤ëŠ˜ì˜ ì´ì•¼ê¸°: {text}{cues_text}"},
            ],
        )
        content = resp.choices[0].message.content.strip()
        # Try to find JSON
        if "```" in content:
            content = content.split("```")[-2]
        result = json.loads(content)
        # guard defaults
        result.setdefault("emotions", ["ì¤‘ë¦½"])  
        result.setdefault("stress_level", 30)
        result.setdefault("energy_level", 50)
        result.setdefault("mood_score", 0)
        result.setdefault("summary", "ì¼ë°˜ì ì¸ ìƒíƒœì…ë‹ˆë‹¤.")
        result.setdefault("keywords", [])
        result.setdefault("tone", "ì¤‘ë¦½ì ")
        result.setdefault("confidence", 0.7)
        return result
    except Exception:
        return analyze_text_simulation(text)


def analyze_text_simulation(text: str) -> Dict:
    text_l = text.lower()
    pos_kw = ["ì¢‹", "í–‰ë³µ", "ë¿Œë“¯", "ê¸°ì¨", "ì¦ê²", "í‰ì˜¨", "ë§Œì¡±"]
    neg_kw = ["í˜ë“¤", "ë¶ˆì•ˆ", "ê±±ì •", "ì§œì¦", "í™”", "ìš°ìš¸", "ìŠ¬í””"]

    pos = sum(k in text_l for k in pos_kw)
    neg = sum(k in text_l for k in neg_kw)

    if pos > neg:
        tone = "ê¸ì •ì "; stress = max(10, 40 - 8 * pos); energy = min(85, 50 + 10 * pos)
    elif neg > pos:
        tone = "ë¶€ì •ì "; stress = min(85, 40 + 10 * neg); energy = max(20, 55 - 8 * neg)
    else:
        tone = "ì¤‘ë¦½ì "; stress = 30; energy = 50

    mood = int(np.clip(energy - stress, -70, 70))
    emos = ["ê¸°ì¨"] if tone == "ê¸ì •ì " else (["ìŠ¬í””"] if tone == "ë¶€ì •ì " else ["ì¤‘ë¦½"])
    return {
        "emotions": emos,
        "stress_level": int(stress),
        "energy_level": int(energy),
        "mood_score": int(mood),
        "summary": f"{tone} ìƒíƒœë¡œ ë³´ì…ë‹ˆë‹¤.",
        "keywords": [],
        "tone": tone,
        "confidence": 0.55,
    }

# =============================
# Fusion: Text (anchor) + Voice Cues (aux)
# =============================

def combine_text_and_voice(text_analysis: Dict, voice_analysis: Optional[Dict]) -> Dict:
    if not voice_analysis or "voice_cues" not in voice_analysis:
        return text_analysis

    cues = voice_analysis["voice_cues"]
    quality = float(cues.get("quality", 0.5))

    # base impact scaled by quality and tone
    base_alpha = 0.25 * quality
    tone = text_analysis.get("tone", "ì¤‘ë¦½ì ")
    if tone == "ê¸ì •ì ":
        base_alpha *= 0.6
    elif tone == "ë¶€ì •ì ":
        base_alpha *= 0.9

    MAX_DS, MAX_DE, MAX_DM = 12, 12, 10
    stress = text_analysis.get("stress_level", 30)
    energy = text_analysis.get("energy_level", 50)
    mood = text_analysis.get("mood_score", 0)

    delta_energy = base_alpha * ((cues["arousal"] - 50) / 50.0) * 12
    delta_stress = base_alpha * (((cues["tension"] - 50) / 50.0) * 12 - ((cues["stability"] - 50) / 50.0) * 6)
    delta_mood = base_alpha * ((cues["stability"] - 50) / 50.0) * 8 - base_alpha * ((cues["tension"] - 50) / 50.0) * 6

    delta_stress = float(np.clip(delta_stress, -MAX_DS, MAX_DS))
    delta_energy = float(np.clip(delta_energy, -MAX_DE, MAX_DE))
    delta_mood = float(np.clip(delta_mood, -MAX_DM, MAX_DM))

    combined = dict(text_analysis)
    combined["stress_level"] = int(np.clip(stress + delta_stress, 0, 100))
    combined["energy_level"] = int(np.clip(energy + delta_energy, 0, 100))
    combined["mood_score"] = int(np.clip(mood + delta_mood, -70, 70))
    combined["confidence"] = float(np.clip(text_analysis.get("confidence", 0.7) + 0.12 * quality, 0, 1))
    combined["voice_analysis"] = voice_analysis
    return combined

# =============================
# Whisper Transcription (optional)
# =============================

def transcribe_audio(audio_bytes: bytes) -> Optional[str]:
    if not openai_client:
        return None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(audio_bytes)
            p = tmp.name
        with open(p, "rb") as fh:
            out = openai_client.audio.transcriptions.create(model="whisper-1", file=fh, language="ko")
        try:
            os.unlink(p)
        except Exception:
            pass
        return out.text
    except Exception:
        return None

# =============================
# Baseline Update (fast and simple)
# =============================

def update_baseline(vf: Dict):
    keys = ["pitch_mean", "tempo", "energy_mean", "hnr", "spectral_centroid_mean"]
    b = st.session_state.prosody_baseline
    # keep running mean with count (store count inside baseline)
    count = int(b.get("_count", 0))
    new_count = min(20, count + 1)  # cap to avoid drift
    alpha = 1.0 / new_count
    for k in keys:
        v = float(vf.get(k, 0.0))
        prev = float(b.get(k, v))
        b[k] = (1 - alpha) * prev + alpha * v
    b["_count"] = new_count

# =============================
# Sidebar: System Status & Nav
# =============================
with st.sidebar:
    st.markdown("### ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ")
    librosa = get_librosa()
    parselmouth = get_parselmouth()

    st.markdown(f"- {'âœ…' if openai_client else 'âš ï¸'} OpenAI API")
    st.markdown(f"- {'âœ…' if librosa else 'âš ï¸'} ìŒì„± ë¶„ì„(Librosa)")
    st.markdown(f"- {'âœ…' if parselmouth else 'â„¹ï¸'} ê³ ê¸‰ ìŒì„±í•™(Praat)")

    if not openai_client:
        with st.expander("ğŸ”‘ OpenAI API í‚¤ ì…ë ¥"):
            api_key = st.text_input("OpenAI API í‚¤", type="password")
            if st.button("ì €ì¥"):
                if api_key.startswith("sk-"):
                    st.session_state.openai_api_key = api_key
                    st.success("API í‚¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒë‹¨ Rerun ë²„íŠ¼ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨ í•´ì£¼ì„¸ìš”.")
                else:
                    st.error("ì˜¬ë°”ë¥¸ í‚¤ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    st.markdown("---")
    page = st.selectbox(
        "í˜ì´ì§€",
        ["ğŸ™ï¸ ì˜¤ëŠ˜ì˜ ì´ì•¼ê¸°", "ğŸ’– ë§ˆìŒ ë¶„ì„", "ğŸ“ˆ ê°ì • ì—¬ì •", "ğŸµ ëª©ì†Œë¦¬ ë³´ì¡°ì§€í‘œ", "ğŸ“š ë‚˜ì˜ ì´ì•¼ê¸°ë“¤"],
    )

# =============================
# Main Pages
# =============================
extractor = VoiceFeatureExtractor()

def today_key() -> str:
    return datetime.now().strftime("%Y-%m-%d")

if page == "ğŸ™ï¸ ì˜¤ëŠ˜ì˜ ì´ì•¼ê¸°":
    st.header("ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë– ì…¨ë‚˜ìš”?")

    # Input widgets
    audio_val = st.audio_input("ğŸ¤ ë§ˆìŒì„ í¸í•˜ê²Œ ë§í•´ë³´ì„¸ìš”", help="ë…¹ìŒ í›„ ì—…ë¡œë“œ")
    text_input = st.text_area(
        "âœï¸ ê¸€ë¡œ í‘œí˜„í•´ë„ ì¢‹ì•„ìš”",
        placeholder="ì˜¤ëŠ˜ì˜ ì´ì•¼ê¸°ë¥¼ ì ì–´ì£¼ì„¸ìš”...",
        height=120,
    )

    if st.button("ğŸ’ ë¶„ì„í•˜ê³  ì €ì¥", type="primary"):
        diary_text = text_input.strip()
        voice_analysis = None
        audio_b64 = None

        if audio_val is not None:
            audio_bytes = audio_val.read()
            audio_b64 = base64.b64encode(audio_bytes).decode()

            with st.spinner("ğŸµ ëª©ì†Œë¦¬ ì‹ í˜¸ë¥¼ ê³„ì‚°í•˜ëŠ” ì¤‘..."):
                vf = extractor.extract(audio_bytes)
                update_baseline(vf)
                voice_analysis = analyze_voice_as_cues(vf, st.session_state.prosody_baseline)

            # Whisper (optional, fast-fail)
            if openai_client and not diary_text:
                with st.spinner("ğŸ¤– ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘..."):
                    tx = transcribe_audio(audio_bytes)
                    if tx:
                        diary_text = tx
                        st.info(f"ë“¤ì€ ì´ì•¼ê¸°: {tx}")

        if not diary_text:
            st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ìŒì„±ì„ ë…¹ìŒí•´ ì£¼ì„¸ìš”.")
        else:
            # Text analysis first (anchor). Pass cues only for context string, not to change labels.
            cues_for_prompt = voice_analysis["voice_cues"] if voice_analysis else None
            with st.spinner("ğŸ¤– í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°ì • ë¶„ì„ ì¤‘..."):
                t_res = analyze_text_with_llm(diary_text, cues_for_prompt)

            # Fuse with voice cues (aux)
            final = combine_text_and_voice(t_res, voice_analysis)

            entry = {
                "id": len(st.session_state.diary_entries) + 1,
                "date": today_key(),
                "time": datetime.now().strftime("%H:%M"),
                "text": diary_text,
                "analysis": final,
                "audio_data": audio_b64,
            }
            st.session_state.diary_entries.append(entry)

            st.success("ğŸ‰ ì†Œì¤‘í•œ ì´ì•¼ê¸°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

            # Quick summary cards (lightweight UI)
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
                st.subheader("ğŸ’– ê°ì • (í…ìŠ¤íŠ¸ ê¸°ë°˜)")
                st.write(", ".join(final.get("emotions", [])))
                st.caption("ê°ì • ë¼ë²¨ì€ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ íŒì •í•©ë‹ˆë‹¤.")
                st.markdown("</div>", unsafe_allow_html=True)
            with col2:
                st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
                st.subheader("ğŸ“Š ë§ˆìŒ ìƒíƒœ")
                st.metric("ìŠ¤íŠ¸ë ˆìŠ¤", f"{final['stress_level']}%")
                st.metric("í™œë ¥", f"{final['energy_level']}%")
                st.markdown("</div>", unsafe_allow_html=True)
            with col3:
                st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
                st.subheader("ğŸ¯ ì»¨ë””ì…˜")
                st.metric("ë§ˆìŒ ì ìˆ˜", f"{final['mood_score']}")
                conf = final.get("confidence", 0.6)
                st.metric("ë¶„ì„ ì‹ ë¢°ë„", f"{conf:.2f}")
                st.markdown("</div>", unsafe_allow_html=True)

            if voice_analysis:
                st.markdown("### ğŸµ ëª©ì†Œë¦¬ ì‹ í˜¸ (ë³´ì¡° ì§€í‘œ)")
                cues = final["voice_analysis"]["voice_cues"]
                qtxt = "ë†’ìŒ" if cues["quality"] > 0.7 else ("ë³´í†µ" if cues["quality"] > 0.4 else "ë‚®ìŒ")
                c1, c2, c3, c4 = st.columns(4)
                c1.metric("ê°ì„±ë„", f"{int(cues['arousal'])}/100")
                c2.metric("ê¸´ì¥ë„", f"{int(cues['tension'])}/100")
                c3.metric("ì•ˆì •ë„", f"{int(cues['stability'])}/100")
                c4.metric("ë…¹ìŒ í’ˆì§ˆ", qtxt)
                st.caption("â€» ëª©ì†Œë¦¬ ì‹ í˜¸ëŠ” ë³´ì¡° ì§€í‘œì…ë‹ˆë‹¤. ê°ì • íŒë‹¨ì€ í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•©ë‹ˆë‹¤.")

elif page == "ğŸµ ëª©ì†Œë¦¬ ë³´ì¡°ì§€í‘œ":
    st.header("ìµœê·¼ ë…¹ìŒì˜ ë³´ì¡°ì§€í‘œ ìƒì„¸")
    entries = [e for e in st.session_state.diary_entries if e.get("analysis", {}).get("voice_analysis")]
    if not entries:
        st.info("ìŒì„±ìœ¼ë¡œ ê¸°ë¡ëœ í•­ëª©ì´ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")
    else:
        latest = entries[-1]
        voice = latest["analysis"]["voice_analysis"]
        vf = voice["voice_features"]
        cues = voice["voice_cues"]
        qtxt = "ë†’ìŒ" if cues["quality"] > 0.7 else ("ë³´í†µ" if cues["quality"] > 0.4 else "ë‚®ìŒ")

        st.subheader("ë³´ì¡°ì§€í‘œ")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("ê°ì„±ë„", f"{int(cues['arousal'])}/100")
        c2.metric("ê¸´ì¥ë„", f"{int(cues['tension'])}/100")
        c3.metric("ì•ˆì •ë„", f"{int(cues['stability'])}/100")
        c4.metric("ë…¹ìŒ í’ˆì§ˆ", qtxt)

        st.subheader("ê¸°ì´ˆ ìŒì„± íŠ¹ì„±")
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("í”¼ì¹˜ í‰ê· (Hz)", f"{vf.get('pitch_mean',0):.1f}")
        c2.metric("ì—ë„ˆì§€(mean)", f"{vf.get('energy_mean',0):.3f}")
        c3.metric("ë§í•˜ê¸° ì†ë„(BPM)", f"{vf.get('tempo',0):.0f}")
        c4.metric("HNR", f"{vf.get('hnr',0):.1f}")
        st.caption("â€» ì´ ìˆ˜ì¹˜ëŠ” ìƒíƒœ ë‹¨ì„œë¡œë§Œ ì‚¬ìš©ë˜ë©° ê°ì • ë¼ë²¨ì„ ì§ì ‘ ê²°ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

elif page == "ğŸ’– ë§ˆìŒ ë¶„ì„":
    st.header("ë§ˆìŒ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    if not st.session_state.diary_entries:
        st.info("ê¸°ë¡ì´ ì•„ì§ ì—†ì–´ìš”.")
    else:
        df = pd.DataFrame([
            {
                "date": e["date"],
                "time": e["time"],
                "emotions": ", ".join(e["analysis"].get("emotions", [])),
                "stress": e["analysis"].get("stress_level", 0),
                "energy": e["analysis"].get("energy_level", 0),
                "mood": e["analysis"].get("mood_score", 0),
                "tone": e["analysis"].get("tone", "ì¤‘ë¦½ì "),
                "confidence": e["analysis"].get("confidence", 0.6),
            }
            for e in st.session_state.diary_entries
        ])
        st.dataframe(df, use_container_width=True, hide_index=True)

elif page == "ğŸ“ˆ ê°ì • ì—¬ì •":
    st.header("ì‹œê°„ì— ë”°ë¥¸ ë³€í™”")
    if not st.session_state.diary_entries:
        st.info("ê¸°ë¡ì´ ìŒ“ì´ë©´ ì¶”ì„¸ë¥¼ ë³´ì—¬ë“œë¦´ê²Œìš”.")
    else:
        df = pd.DataFrame([
            {
                "dt": f"{e['date']} {e['time']}",
                "stress": e["analysis"].get("stress_level", 0),
                "energy": e["analysis"].get("energy_level", 0),
                "mood": e["analysis"].get("mood_score", 0),
            }
            for e in st.session_state.diary_entries
        ])
        st.line_chart(df.set_index("dt"))
        st.caption("â€» ê°„ë‹¨í•œ ê¸°ë³¸ ì°¨íŠ¸ë¡œ í‘œì‹œí•©ë‹ˆë‹¤ (ì„±ëŠ¥ì„ ìœ„í•´ ë‚´ì¥ ì°¨íŠ¸ ì‚¬ìš©).")

elif page == "ğŸ“š ë‚˜ì˜ ì´ì•¼ê¸°ë“¤":
    st.header("ì•„ì¹´ì´ë¸Œ")
    if not st.session_state.diary_entries:
        st.info("ì•„ì§ ê¸°ë¡ëœ ì´ì•¼ê¸°ê°€ ì—†ì–´ìš”.")
    else:
        for e in reversed(st.session_state.diary_entries[-20:]):  # ìµœê·¼ 20ê°œë§Œ ë Œë” (í¼í¬ë¨¼ìŠ¤)
            with st.expander(f"{e['date']} {e['time']} Â· {', '.join(e['analysis'].get('emotions', []))}"):
                st.write(e["text"])
                st.json({k: e["analysis"][k] for k in ["stress_level", "energy_level", "mood_score", "tone", "confidence"]})

# =============================
# Sidebar: Export / Backup / Reset
# =============================
with st.sidebar:
    if st.session_state.diary_entries:
        st.markdown("---")
        if st.button("ğŸ“ CSV ë‚´ë³´ë‚´ê¸°"):
            rows = []
            for e in st.session_state.diary_entries:
                a = e["analysis"]
                row = {
                    "date": e["date"],
                    "time": e["time"],
                    "text": e["text"],
                    "emotions": ", ".join(a.get("emotions", [])),
                    "stress": a.get("stress_level", 0),
                    "energy": a.get("energy_level", 0),
                    "mood": a.get("mood_score", 0),
                    "tone": a.get("tone", "ì¤‘ë¦½ì "),
                    "confidence": a.get("confidence", 0.6),
                }
                if a.get("voice_analysis"):
                    v = a["voice_analysis"]
                    vc = v["voice_cues"]
                    vf = v["voice_features"]
                    row.update(
                        {
                            "arousal": vc.get("arousal", ""),
                            "tension": vc.get("tension", ""),
                            "stability": vc.get("stability", ""),
                            "quality": vc.get("quality", ""),
                            "pitch_mean": vf.get("pitch_mean", ""),
                            "energy_mean": vf.get("energy_mean", ""),
                            "tempo": vf.get("tempo", ""),
                            "hnr": vf.get("hnr", ""),
                        }
                    )
                rows.append(row)
            df = pd.DataFrame(rows)
            csv = df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button("ë‹¤ìš´ë¡œë“œ", csv, file_name=f"voice_diary_{datetime.now().strftime('%Y%m%d')}.csv", mime="text/csv")

        if st.button("ğŸ—‘ï¸ ëª¨ë“  ê¸°ë¡ ì‚­ì œ"):
            st.session_state.diary_entries = []
            st.success("ëª¨ë“  ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

# =============================
# Footer
# =============================
st.markdown("---")
st.caption("Made with â¤ï¸ Â· ê°ì • ë¼ë²¨ì€ í…ìŠ¤íŠ¸ ìš°ì„  Â· ëª©ì†Œë¦¬ëŠ” ê°ì„±/ê¸´ì¥/ì•ˆì •ì˜ ë³´ì¡° ì§€í‘œë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
